# Gradient-Descent-Algorithm-Impl
Scratch implementation of Gradient descent algorithm without any use of existing machine learning libraries

Gradient Descent is used while training a machine learning model. It is an optimization algorithm, based on a convex function, that tweaks itâ€™s parameters iteratively to minimize a given function to its local minimum. It is simply used to find the values of a functions parameters (coefficients) that minimize a cost function as far as possible.

Initial parameters values should be defiened first and from there on Gradient Descent iteratively adjusts the values, using calculus, so that they minimize the given cost-function. 

A gradient measures the change in all weights with regard to the change in error. You can also think of a gradient as the slope of a function. The higher the gradient, the steeper the slope and the faster a model can learn. But if the slope is zero, the model stops learning. Said it more mathematically, a gradient is a partial derivative with respect to its inputs.

